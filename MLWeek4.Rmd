---
title: "Prediction Assignment - Week 4 ML"
output: html_document
fig_caption: yes
author: Tony O'Donoghue
date: 8th June 2016
---

Introduction
=============
Human Activity Recognition has emerged as an important research area in  recent years. With devices such as *Fitbit*, *Nike FuelBand*, and *Jawbone Up* it is now possible to collect inexpensively  a large amount of data about personal activity. For this assignmnet data was collected from six young people who were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Measurements were taken form all 6 participants from accelerometers attached on the belt, forearm, arm, and dumbell.  

Data Sources
------------
- Training Data for this assign ment is available at <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>
  
- Test Data for this assignment is available at <https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>

Objective
----------
Generate a model or models from the predictors in the training dataset which can predict the **classe** variable.
The selected final model will then be used to predict using the test data.

Data Loading & Feature Extraction
----------------------------------

```{r, warning=FALSE,message=FALSE}
#Load Libraries
library(e1071)
library(lattice)
library(ggplot2)
library(caret)
library(fields)
library(knitr)

```
  
  
```{r,eval=FALSE}
#Download Files
setwd("C:/MyProjects/Coursera_R/Week4Assignment/")

trainFileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(trainFileUrl,destfile = "./pml-training.csv")

testFileUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(testFileUrl,destfile = "./pml-testing.csv")
```
  
```{r}  
#Load Data Frames
training<- read.csv("pml-training.csv", na.strings= c("", "NA", "#DIV/0!"))
finalTesting<- read.csv("pml-testing.csv", na.strings= c("", "NA", "#DIV/0!"))
```

We will use **60%** of the training data for actual training and **40%** for model testing and evaluation.
```{r}
set.seed(1234)
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
trainingForBuild <- training[inTrain, ]
testingForBuild <- training[-inTrain, ]
```

First seven columns only contains time-stamp, first name, and sequence related data which are not actual movement measurements and are therefore not required.

```{r}
# Remove first 7 columns
trainingForBuild <- trainingForBuild[, -(1:7)]
```

The "near zero-variance predictors" can cause numerical problems during resampling for some models. The following criteria defines these values:  
1. the percentage of unique values is less than 20% and   
2. the ratio of the most frequent to the second most frequent value is greater than 20.  
Also, remove columns with little or no data (NA greater than 49%).  
Use remaining columns for training and testing.
``` {r}

# Remove near zero variance features
nzvcol <- nearZeroVar(trainingForBuild)
trainingForBuild <- trainingForBuild[, -nzvcol]

# Remove Cols with little or no data
trainColStats<-t(stats(trainingForBuild))
rowCount <- dim(trainingForBuild)[1]

#classe has NA from stats but do not exclude that
colsToRemove<-apply(trainColStats, 1, 
               function(currow)
                {
                   
                (currow["missing values"]/rowCount > 0.49) &&      !(is.na(currow))
                })


trainingForBuild<-trainingForBuild[!colsToRemove]

testingForBuild<-testingForBuild[,names(trainingForBuild)]

```

Final Features
--------------
```{r}
dim(trainingForBuild)
```

```{r}
dim(testingForBuild)
```

Random Forest Model
-------------------

One big  advantage of Random Forest is that they do not expect linear features and the features do not have to interact linearly.Random forests builds large collection of de-correlated trees and then averages them.

**Cross Validation**

Random Forest model has a tuning parameter *mtry* - the number of randomly selected predictors used for each node split. Cross-validation is used to automatically pick this parameter. For now have limited the number of trees to 100 and set number of folds to 5.  
```{r,cache=TRUE, warning=FALSE,message=FALSE}
set.seed(1234)

system.time(fitRf <- train(classe ~ ., data=trainingForBuild,  method= "rf",ntree = 100, trControl = trainControl(method ="cv", number = 5)))

#Can plot CV error against mtry using:
#plot(fitRf)
fitRf$finalModel
```
After training out of sample error estimate is only 1.04%.  
  
Evaluation of Random Forest against test data:  
```{r, warning=FALSE,message=FALSE}
preditFitRf <- predict(fitRf, newdata = testingForBuild)
conRf <- confusionMatrix(preditFitRf, testingForBuild$classe)
RfAccuracy <-conRf$overall["Accuracy"]

misClassification = function(v, p) {
    sum(p != v)/length(v)
}
classErrorRate <- misClassification (testingForBuild$classe, preditFitRf)

conRf
```

Random Forest Out Of Sample Error Rate 
---------------------------------------
The accuracy of the Random Forest is `r RfAccuracy`. Actual misclassification error rate on test data is `r classErrorRate * 100 `%.  
The graph below shows how the training error was reduced as the number of trees grew.
```{r Rf_Training, fig.width=8, fig.height=4}
plot(fitRf$finalMode, main="Error Rate vs. Number of Trees")
```

```{r Rf_VarImp, fig.width=6, fig.height=4}
#if you set importance=T in the training set can get more detaild plot of importance.
plot(varImp(fitRf), top = 10, main="Overall Variable Importance across all classes")
```

Stochastic Gradient Boosting
------------------------------
Boosting trees are grown sequentially. Each tree is grown using information from the previously grown tree. Each new tree is fitted using the residuals of the model. There is a parameter d which sets each tree terminal nodes and a shrinkage parameter lambda.

**Cross Validation**

```{r,cache=TRUE, warning=FALSE,message=FALSE}
set.seed(1234)
myTuneGrid <- expand.grid(n.trees = seq(50,250,50),interaction.depth = 1:3,shrinkage = 0.1, n.minobsinnode = 10)
system.time(fitGbm <- train(classe ~ ., data=trainingForBuild, method= "gbm", verbose = F, tuneGrid=myTuneGrid, trControl = trainControl(method ="cv", number = 5)))

fitGbm
```
Cross vaidation results picked n.trees = 250 and interaction.depth = 3. Held constant were Shrinkage = 0.1 and n.minobsinnode = 10.  Here a *tuneGrid* object is created with max tree of 250. The Boosting Model gets more accurate as he number of trees increase.  

Evaluation Boosting Model against test data:  

```{r, warning=FALSE,message=FALSE}
preditFitGbm <- predict(fitGbm, newdata = testingForBuild)
conGbm <- confusionMatrix(preditFitGbm, testingForBuild$classe)
GbmAccuracy <-conGbm$overall["Accuracy"]

classErrorRateGbm <- misClassification (testingForBuild$classe, preditFitGbm)

conGbm
```

Boosting Out Of Sample Error Rate 
---------------------------------------
The accuracy of the Boosting Model is `r GbmAccuracy`. Actual misclassification error rate on test data is `r classErrorRateGbm * 100 `%.

```{r Gbm_Training,  fig.width=6, fig.height=4 }
plot(fitGbm)
fitGbm$finalModel
```

```{r Gbm_VarImp,  fig.width=6, fig.height=4 }
plot(varImp(fitGbm), top = 10)

```

Quadratic Discriminant Analysis
---------------------------------
QDA has no tuning parameters, so no cross-validtion will be carried out for tuning purposes. By default Bootstraping is used for resampling and gives an estimated Accuracy of 0.8920098.


```{r,cache=TRUE, warning=FALSE, message=FALSE}

set.seed(1234)
system.time(fitQda <- train(classe ~ ., data=trainingForBuild, method= "qda"))

fitQda

```

Evaluation QDA Model against test data:  

```{r, warning=FALSE,message=FALSE}
preditFitQda <- predict(fitQda, newdata = testingForBuild)
conQda <- confusionMatrix(preditFitQda, testingForBuild$classe)
QdaAccuracy <-conQda$overall["Accuracy"]

classErrorRateQda <- misClassification (testingForBuild$classe, preditFitQda)

conQda
```
QDA Out Of Sample Error Rate 
---------------------------------------
The accuracy of the Quadratic Discriminant Analysis Model is `r QdaAccuracy`. Actual misclassification error rate on test data is `r classErrorRateQda * 100 `%.

Conclusion
==========

The Random Forest model is the most accurate and will use it for final test set. For this assignment the Random Forest model was well suited and it produced excellent results on the training/validation and test sets.  
- It gives estimates of what variables are important in the classification.  
- Gives good perfromance on a large dataset with many predictors.  
- with caret and cross-validation can automtically select tuning parameters.  

For this exercise Boosting could potentially be as accurate but the interaction depth and number of trees would need to be increased, and thus would take much longer to train.  
  
Quadratic Discriminant Analysis was the least accurate but was by far the quickest to train.
  


```{r}
preditFitRfFinal <- predict(fitRf, newdata = finalTesting)
preditFitGbmFinal <- predict(fitGbm, newdata = finalTesting)
preditFitQdaFinal <- predict(fitQda, newdata = finalTesting)

preditFitRfFinal
preditFitGbmFinal 
preditFitQdaFinal
```
